# Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding

Original Repo can be viewed [here](https://github.com/DAMO-NLP-SG/Video-LLaMA)

This is the repo for the revised version of Video-LLaMA for testing on [MMTOM-QA](https://openreview.net/forum?id=sMFqEror1b) dataset. 

## Tutorial


## Citation
If you find our project useful, hope you can star our repo and cite our paper as follows:
```
@inproceedings{
anonymous2023mmtomqa,
title={{MMT}oM-{QA}: Multimodal Theory of Mind Question Answering},
author={Anonymous},
booktitle={Submitted to The Twelfth International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=sMFqEror1b},
note={under review}
}
```

